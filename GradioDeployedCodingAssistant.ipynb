{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkRXFOxCxve_"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PQ7AVEaxd3k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiKEwKPWyO8U"
      },
      "source": [
        "# Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzHw2swQxuk5"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(\"hf://datasets/openai/openai_humaneval/openai_humaneval/test-00000-of-00001.parquet\")\n",
        "df = df.iloc[:,:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpBhzI0WAJwh"
      },
      "source": [
        "# PART 1 : RAG SYSTEM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpKsS1oHySZS"
      },
      "source": [
        "# Setting up VectorDB using Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYOxYu_FyAHz"
      },
      "outputs": [],
      "source": [
        "# Setting up embeddingsee\n",
        "embeddings = HuggingFaceEmbeddings(model_name = \"all-mpnet-base-v2\")\n",
        "\n",
        "# Setting up data for compatibility with Chroma.from_texts\n",
        "prompts = [prompt for prompt in df[\"prompt\"]]\n",
        "metadata = [{\"solution\": sol} for sol in df[\"canonical_solution\"]]\n",
        "\n",
        "#Creating Vectorstore\n",
        "vectorstore = Chroma.from_texts(\n",
        "    texts = prompts,\n",
        "    embedding = embeddings,\n",
        "    metadatas = metadata,\n",
        "    ids = df[\"task_id\"],\n",
        "    persist_directory=\"./chroma_store\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNHcXWs6ygUs"
      },
      "source": [
        "# Retrieval Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnC5yj-JypsF"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def retrieveDocsFromQuery(query : str, k : int = 4, printResults : bool = False) -> Tuple[List, List]:\n",
        "  \"\"\"Takes in a query and returns K most relevant tasks.\"\"\"\n",
        "\n",
        "  retriever = vectorstore.as_retriever(\n",
        "    search_type = \"similarity\",\n",
        "    search_kwargs = {\"k\" : k}\n",
        "  )\n",
        "  tasks  = []\n",
        "  solutions = []\n",
        "  for i, doc in enumerate(retriever.invoke(query)):\n",
        "    tasks.append(doc.page_content)\n",
        "    solutions.append(doc.metadata[\"solution\"])\n",
        "\n",
        "    if printResults:\n",
        "      print(f\"\\n==========\\nTASK {i+1} IN DATABASE\\n==========\\n{tasks[i]}\", '\\n')\n",
        "      print(f\"\\n==========\\nSOLUTION {i+1} IN DATABASE\\n==========\\n{solutions[i]}\")\n",
        "\n",
        "  return tasks, solutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TSHLFg6AEKL"
      },
      "source": [
        "# PART 2 : LANGGRAPH AGENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as6hlKcfARpC"
      },
      "source": [
        "# Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpODnk7aAUWG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
        "model     = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\",\n",
        "                                            torch_dtype=torch.float16,\n",
        "                                            device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0243BLe6CNhD"
      },
      "source": [
        "# LangGraph Python Assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyfTH-ETHQVH"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import TypedDict, Union, List, Dict\n",
        "from langchain_core.messages import HumanMessage, AIMessage, FunctionMessage, SystemMessage\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.messages.utils import convert_to_openai_messages\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBGweqZJuD6l"
      },
      "outputs": [],
      "source": [
        "def generateCodeFromMsg(msg, max_new_tokens = 500):\n",
        "  msg_converted = convert_to_openai_messages(msg)\n",
        "\n",
        "  inputs = processor.apply_chat_template(\n",
        "        msg_converted,\n",
        "        add_generation_prompt=True,   # append the “now you speak” token(s)\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "  outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "  start = inputs[\"input_ids\"].shape[-1]\n",
        "  response  = processor.decode(outputs[0][start:], skip_special_tokens=True)\n",
        "\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htHxXw56IGaO"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages : List[Union[HumanMessage, AIMessage, SystemMessage]]\n",
        "  chat_state : str\n",
        "\n",
        "def chat_node(state : AgentState) -> AgentState:\n",
        "\n",
        "  response = generateCodeFromMsg(state[\"messages\"])\n",
        "\n",
        "  state[\"messages\"].append(AIMessage(content = response))\n",
        "  print(f\"\\nAI: '{response}'\")\n",
        "\n",
        "  return state\n",
        "\n",
        "def explain_node(state : AgentState) -> AgentState:\n",
        "\n",
        "  state[\"messages\"] += [SystemMessage(\"\"\"You are a helpful assistant that explains code in simple terms.\n",
        "                                        However, if no code is provided, you may respond normally\"\"\")]\n",
        "\n",
        "  response = generateCodeFromMsg(state[\"messages\"])\n",
        "\n",
        "  state[\"messages\"].append(AIMessage(content = response))\n",
        "  print(f\"\\nAI: '{response}'\")\n",
        "\n",
        "  return state\n",
        "\n",
        "def generate_node(state : AgentState) -> AgentState:\n",
        "\n",
        "  state[\"messages\"] += [SystemMessage(\"You are a coding assistant. Generate code that fulfills the user's request.\")]\n",
        "\n",
        "  response = generateCodeFromMsg(state[\"messages\"])\n",
        "\n",
        "  state[\"messages\"].append(AIMessage(content = response))\n",
        "  print(f\"\\nAI: {response}\")\n",
        "  return state\n",
        "\n",
        "\n",
        "def router(state : AgentState):\n",
        "  if state[\"chat_state\"] == \"chat_normally\":\n",
        "    return \"chat_edge\"\n",
        "  elif state[\"chat_state\"] == \"explain_code\":\n",
        "    return \"explain_edge\"\n",
        "  elif state[\"chat_state\"] == \"generate_code\":\n",
        "    return \"generate_edge\"\n",
        "\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"chat_node\", chat_node)\n",
        "graph.add_node(\"explain_node\", explain_node)\n",
        "graph.add_node(\"generate_node\", generate_node)\n",
        "graph.add_node(\"router\", lambda state : state)\n",
        "\n",
        "graph.add_conditional_edges(\n",
        "    source = \"router\",\n",
        "    path = router,\n",
        "    path_map = {\n",
        "        \"chat_edge\" : \"chat_node\",\n",
        "        \"explain_edge\" : \"explain_node\",\n",
        "        \"generate_edge\" : \"generate_node\"\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "graph.add_edge(START, \"router\")\n",
        "graph.add_edge(\"chat_node\", END)\n",
        "graph.add_edge(\"explain_node\", END)\n",
        "graph.add_edge(\"generate_node\", END)\n",
        "\n",
        "agent = graph.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aCPcgyOCZN0",
        "outputId": "50c511fe-ab11-437d-9f7b-8f269fc709da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WELCOME TO PHI 3 CODING ASSISTANT: Type 'new' at any time to start a new Chat. Type 'exit' to exit program.\n",
            "Enter: exit\n"
          ]
        }
      ],
      "source": [
        "conversation_history = []\n",
        "operation = \"chat\"\n",
        "print(\"WELCOME TO PHI 3 CODING ASSISTANT: Type 'new' at any time to start a new Chat. Type 'exit' to exit program.\")\n",
        "\n",
        "while True:\n",
        "  user_input = input(\"Enter: \")\n",
        "  if user_input == \"exit\": break\n",
        "\n",
        "  print(f\"\\n\\nUser: {user_input}\")\n",
        "  if(user_input.lower() == \"new\"):\n",
        "    conversation_history = []\n",
        "    docs = ''\n",
        "    continue\n",
        "\n",
        "  # SMART ROUTING IMPLEMENATION\n",
        "  intent_classifier_prompt = f\"\"\"\n",
        "    You are an intent‑classifier. Read the user’s message and respond *only* with JSON in this exact schema:\n",
        "    {{\n",
        "      \"task\": <one of: \"explain_code\", \"generate_code\", \"chat_normally\">,\n",
        "      \"user_input\": <the original user text, verbatim>\n",
        "    }}\n",
        "\n",
        "    Examples:\n",
        "    User: \"Can you walk me through what this function does line by line?\"\n",
        "    ➞ {{\"task\":\"explain_code\",\"user_input\":\"Can you walk me through what this function does line by line?\"}}\n",
        "\n",
        "    User: \"Write me a Python script that parses a CSV and prints the average of column A.\"\n",
        "    ➞ {{\"task\":\"generate_code\",\"user_input\":\"Write me a Python script that parses a CSV and prints the average of column A.\"}}\n",
        "\n",
        "    User: \"Hey, how’s your day going?\"\n",
        "    ➞ {{\"task\":\"chat_normally\",\"user_input\":\"Hey, how’s your day going?\"}}\n",
        "\n",
        "    Now classify:\n",
        "    User: \"{user_input}\"\n",
        "    \"\"\"\n",
        "  input_to_intent_classifier = processor.apply_chat_template(\n",
        "      convert_to_openai_messages([SystemMessage(intent_classifier_prompt)]),\n",
        "      add_generation_prompt=True,   # append the “now you speak” token(s)\n",
        "      tokenize=True,\n",
        "      return_dict=True,\n",
        "      return_tensors=\"pt\",\n",
        "  ).to(model.device)\n",
        "\n",
        "  output = model.generate(**input_to_intent_classifier, max_new_tokens = 200)\n",
        "  start = input_to_intent_classifier[\"input_ids\"].shape[-1]\n",
        "  response  = processor.decode(output[0][start:], skip_special_tokens=True)\n",
        "  print(f\"\\nRESPONSE OF INTENT CLASIFIER IS {response}\\n\")\n",
        "\n",
        "  parsed_response = json.loads(response)\n",
        "\n",
        "  operation = parsed_response[\"task\"]\n",
        "\n",
        "  if operation == \"explain_code\" or operation == \"generate_code\":\n",
        "\n",
        "    tasks,solutions = retrieveDocsFromQuery(user_input, k = 3, printResults = False)\n",
        "\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"Task:\\n{task}\\n\\nSolution:\\n{solution}\"\n",
        "        for task,solution in zip(tasks,solutions)\n",
        "        )\n",
        "    system_msg = (\n",
        "        \"Use the following examples for context (do not just repeat them word‑for‑word):\\n\\n\"\n",
        "        f\"{context}\"\n",
        "        )\n",
        "    conversation_history.append(SystemMessage(system_msg))\n",
        "    conversation_history.append(HumanMessage(user_input))\n",
        "  elif operation == \"chat_normally\":\n",
        "    conversation_history.append(SystemMessage(\"You are a helpful assistant\"))\n",
        "    conversation_history.append(HumanMessage(user_input))\n",
        "  else:\n",
        "    print(\"\\nINCORRECT OPERATION\\n\")\n",
        "    continue\n",
        "\n",
        "  print(f\"Conversation History: {conversation_history}\")\n",
        "  result = agent.invoke({\"messages\" : conversation_history,\n",
        "                         \"chat_state\" : operation})\n",
        "\n",
        "  conversation_history = result[\"messages\"]\n",
        "  operation = result[\"chat_state\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vEfpyu-otJC"
      },
      "source": [
        "# 2. DEPLOYMENT ON GRADIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_JEZT_VgMyB"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet gradio\n",
        "\n",
        "import gradio as gr\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYVFutvJ-AFI"
      },
      "outputs": [],
      "source": [
        "def gradio_step(user_input, chat_history, conv_history, operation):\n",
        "    \"\"\"\n",
        "    - user_input: str, the new user message\n",
        "    - chat_history: list of (user, bot) tuples for display\n",
        "    - conv_history: List[HumanMessage|SystemMessage|AIMessage], your raw transcript\n",
        "    - operation: str, one of \"chat_normally\", \"explain_code\", \"generate_code\"\n",
        "    \"\"\"\n",
        "    # 1) Handle “new” to reset\n",
        "    if user_input.lower().strip() == \"new\":\n",
        "        return [], [], [], \"\"\n",
        "\n",
        "    # 2) Intent classification\n",
        "    intent_prompt = f\"\"\"\n",
        "    You are an intent‑classifier. Read the user’s message and respond *only* with JSON in this exact schema:\n",
        "    {{\n",
        "      \"task\": <one of: \"explain_code\", \"generate_code\", \"chat_normally\">,\n",
        "      \"user_input\": <the original user text, verbatim>\n",
        "    }}\n",
        "\n",
        "    Examples:\n",
        "    User: \"Can you walk me through what this function does line by line?\"\n",
        "    ➞ {{\"task\":\"explain_code\",\"user_input\":\"Can you walk me through what this function does line by line?\"}}\n",
        "\n",
        "    User: \"Write me a Python script that parses a CSV and prints the average of column A.\"\n",
        "    ➞ {{\"task\":\"generate_code\",\"user_input\":\"Write me a Python script that parses a CSV and prints the average of column A.\"}}\n",
        "\n",
        "    User: \"Hey, how’s your day going?\"\n",
        "    ➞ {{\"task\":\"chat_normally\",\"user_input\":\"Hey, how’s your day going?\"}}\n",
        "\n",
        "    Now classify:\n",
        "    User: \"{user_input}\"\n",
        "    \"\"\"\n",
        "    cls_inputs = processor.apply_chat_template(\n",
        "        convert_to_openai_messages([SystemMessage(intent_prompt)]),\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "    cls_out = model.generate(**cls_inputs, max_new_tokens=200)\n",
        "    start = cls_inputs[\"input_ids\"].shape[-1]\n",
        "    cls_resp = processor.decode(cls_out[0][start:], skip_special_tokens=True)\n",
        "\n",
        "    #Parsing JSON safely\n",
        "    try:\n",
        "        parsed = json.loads(cls_resp)\n",
        "        operation = parsed.get(\"task\", \"chat_normally\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"[Warning] Failed to parse intent JSON: {repr(cls_resp)}\")\n",
        "        operation = \"chat_normally\"\n",
        "\n",
        "    # 3) Build up conv_history for RAG\n",
        "    if operation in (\"explain_code\",\"generate_code\"):\n",
        "        tasks, sols = retrieveDocsFromQuery(user_input, k=3, printResults=False)\n",
        "\n",
        "        ctx = \"\\n\\n\".join(f\"Task:\\n{t}\\n\\nSolution:\\n{s}\" for t,s in zip(tasks,sols))\n",
        "        conv_history += [\n",
        "            SystemMessage(f\"Use the following examples for context...\\n\\n{ctx}\"),\n",
        "            HumanMessage(user_input)\n",
        "        ]\n",
        "    else:\n",
        "        conv_history += [\n",
        "            SystemMessage(\"You are a helpful assistant\"),\n",
        "            HumanMessage(user_input)\n",
        "        ]\n",
        "\n",
        "    # 4) Invoke your LangGraph agent\n",
        "    result = agent.invoke({\n",
        "        \"messages\": conv_history,\n",
        "        \"chat_state\": operation\n",
        "    })\n",
        "    conv_history = result[\"messages\"]\n",
        "    operation    = result[\"chat_state\"]\n",
        "\n",
        "    # 5) Extract and append bot reply to chat_history\n",
        "    bot_msg     = conv_history[-1].content\n",
        "    chat_history = chat_history or []\n",
        "    chat_history.append((user_input, bot_msg))\n",
        "\n",
        "    return chat_history, conv_history, operation, \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R0Y_PssEUti",
        "outputId": "b819daca-9c34-478a-c7f6-cc49ab0a58c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-576126707.py:4: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot       = gr.Chatbot()\n"
          ]
        }
      ],
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## PHI3 Coding Assistant\")\n",
        "    chatbot       = gr.Chatbot()\n",
        "    user_in       = gr.Textbox(placeholder=\"Type here and press Enter…\")\n",
        "    conv_history  = gr.State([])               # your raw message list\n",
        "    operation     = gr.State(\"chat_normally\")  # initial routing state\n",
        "\n",
        "    user_in.submit(\n",
        "        fn=gradio_step,\n",
        "        inputs=[user_in, chatbot, conv_history, operation],\n",
        "        outputs=[chatbot, conv_history, operation, user_in],\n",
        "    )\n",
        "\n",
        "# demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOSFvqHIImkP"
      },
      "source": [
        "# RAG Evaluation on MBPP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD9IykGxNSe-"
      },
      "outputs": [],
      "source": [
        "mbpp_df = pd.read_json(\"/content/drive/MyDrive/Colab Notebooks/Cellula-Task5/sanitized-mbpp.json\")\n",
        "mbpp_df = mbpp_df.sample(10, random_state=42).loc[:, [\"prompt\",\"task_id\", \"code\", \"test_list\"]]\n",
        "mbpp_df.set_index(\"task_id\", inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvtEhQr4elvH"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "jpLj2kOdSEHI",
        "outputId": "1fd89dc2-d7df-4314-a004-d6cc74192857"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"mbpp_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"task_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 243,\n        \"min\": 72,\n        \"max\": 802,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          295,\n          127,\n          562\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Write a function to return the sum of all divisors of a number.\",\n          \"Write a function to multiply two integers.\",\n          \"Write a python function to find the length of the longest sublists.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"code\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"def sum_div(number):\\n    divisors = [1]\\n    for i in range(2, number):\\n        if (number % i)==0:\\n            divisors.append(i)\\n    return sum(divisors)\",\n          \"def multiply_int(x, y):\\n    if y < 0:\\n        return -multiply_int(x, -y)\\n    elif y == 0:\\n        return 0\\n    elif y == 1:\\n        return x\\n    else:\\n        return x + multiply_int(x, y - 1)\",\n          \"def Find_Max_Length(lst):  \\n    maxLength = max(len(x) for x in lst )\\n    return maxLength \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "mbpp_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-68c83178-c01a-47cb-bddc-dc6dabcac02b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>code</th>\n",
              "      <th>test_list</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>task_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>802</th>\n",
              "      <td>Write a python function to count the number of rotations required to generate a sorted array. https://www.geeksforgeeks.org/count-of-rotations-required-to-generate-a-sorted-array/</td>\n",
              "      <td>def count_rotation(arr):   \\n    for i in range (1,len(arr)): \\n        if (arr[i] &lt; arr[i - 1]): \\n            return i  \\n    return 0</td>\n",
              "      <td>[assert count_rotation([3,2,1]) == 1, assert count_rotation([4,5,1,2,3]) == 2, assert count_rotation([7,8,9,1,2,3]) == 3, assert count_rotation([1,2,3]) == 0, assert count_rotation([1,3,2]) == 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>Write a function to multiply two integers.</td>\n",
              "      <td>def multiply_int(x, y):\\n    if y &lt; 0:\\n        return -multiply_int(x, -y)\\n    elif y == 0:\\n        return 0\\n    elif y == 1:\\n        return x\\n    else:\\n        return x + multiply_int(x, y - 1)</td>\n",
              "      <td>[assert multiply_int(10,20)==200, assert multiply_int(5,10)==50, assert multiply_int(4,8)==32]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68c83178-c01a-47cb-bddc-dc6dabcac02b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-68c83178-c01a-47cb-bddc-dc6dabcac02b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-68c83178-c01a-47cb-bddc-dc6dabcac02b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e831d9ab-bf42-42a9-87a3-2182fff57122\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e831d9ab-bf42-42a9-87a3-2182fff57122')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e831d9ab-bf42-42a9-87a3-2182fff57122 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                                                                                                                                      prompt  \\\n",
              "task_id                                                                                                                                                                                        \n",
              "802      Write a python function to count the number of rotations required to generate a sorted array. https://www.geeksforgeeks.org/count-of-rotations-required-to-generate-a-sorted-array/   \n",
              "127                                                                                                                                               Write a function to multiply two integers.   \n",
              "\n",
              "                                                                                                                                                                                                              code  \\\n",
              "task_id                                                                                                                                                                                                              \n",
              "802                                                                       def count_rotation(arr):   \\n    for i in range (1,len(arr)): \\n        if (arr[i] < arr[i - 1]): \\n            return i  \\n    return 0   \n",
              "127      def multiply_int(x, y):\\n    if y < 0:\\n        return -multiply_int(x, -y)\\n    elif y == 0:\\n        return 0\\n    elif y == 1:\\n        return x\\n    else:\\n        return x + multiply_int(x, y - 1)   \n",
              "\n",
              "                                                                                                                                                                                                   test_list  \n",
              "task_id                                                                                                                                                                                                       \n",
              "802      [assert count_rotation([3,2,1]) == 1, assert count_rotation([4,5,1,2,3]) == 2, assert count_rotation([7,8,9,1,2,3]) == 3, assert count_rotation([1,2,3]) == 0, assert count_rotation([1,3,2]) == 2]  \n",
              "127                                                                                                           [assert multiply_int(10,20)==200, assert multiply_int(5,10)==50, assert multiply_int(4,8)==32]  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mbpp_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1424HUStR23t"
      },
      "outputs": [],
      "source": [
        "prompts_mbpp = [prompt for prompt in mbpp_df[\"prompt\"]]\n",
        "metadatas_mbpp = [{\"solution\" : code} for code in mbpp_df[\"code\"]]\n",
        "\n",
        "\n",
        "vectorstore2 = Chroma.from_texts(\n",
        "    texts = prompts_mbpp,\n",
        "    embedding = embeddings,\n",
        "    metadatas = metadatas_mbpp,\n",
        "    persist_directory = \"./chroma_store2\"\n",
        ")\n",
        "\n",
        "retriever2 = vectorstore2.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTU4wi3BfB9L",
        "outputId": "84525b8c-30f7-482c-8982-9be73857bc85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def count_rotation(arr):   \n",
            "    for i in range (1,len(arr)): \n",
            "        if (arr[i] < arr[i - 1]): \n",
            "            return i  \n",
            "    return 0\n"
          ]
        }
      ],
      "source": [
        "mbpp_query = prompts_mbpp[0]\n",
        "\n",
        "print(retriever2.invoke(mbpp_query)[0].metadata[\"solution\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIMmv4V0f4H1"
      },
      "outputs": [],
      "source": [
        "def evaluateRAG(query : str):\n",
        "\n",
        "  docs = retriever2.invoke(query)\n",
        "  tasks = []\n",
        "  solutions = []\n",
        "\n",
        "  for doc in docs:\n",
        "    tasks.append(doc.page_content)\n",
        "    solutions.append(doc.metadata[\"solution\"])\n",
        "\n",
        "  context = \"\\n\\n\".join(\n",
        "        f\"Task:\\n{task}\\n\\nSolution:\\n{solution}\"\n",
        "        for task,solution in zip(tasks,solutions)\n",
        "        )\n",
        "\n",
        "  context_msg = (\n",
        "        \"Use the following examples for context:\\n\\n\"\n",
        "        f\"{context}\"\n",
        "        )\n",
        "\n",
        "  instruction_msg = (\n",
        "      f\"\"\"You are an expert python programmer. Here is your task: \\n{query}\\n Respond *only* with the python function.No more. Do NOT add any additional text after the function's end:\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "  msg = []\n",
        "  msg.append(SystemMessage(context_msg))\n",
        "  msg.append(HumanMessage(instruction_msg))\n",
        "\n",
        "  input = processor.apply_chat_template(\n",
        "      convert_to_openai_messages(msg),\n",
        "      add_generation_prompt=True,\n",
        "      tokenize=True,\n",
        "      return_dict=True,\n",
        "      return_tensors=\"pt\",\n",
        "  ).to(model.device)\n",
        "\n",
        "  output = model.generate(**input, max_new_tokens=500)\n",
        "  start = input[\"input_ids\"].shape[-1]\n",
        "  response = processor.decode(output[0][start:], skip_special_tokens=True)\n",
        "\n",
        "  return response\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9bqvBx4iE-8",
        "outputId": "ae96b0f9-03bc-48bc-a328-7ac1bfa2bdd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The Task is \n",
            "Write a python function to count the number of rotations required to generate a sorted array. https://www.geeksforgeeks.org/count-of-rotations-required-to-generate-a-sorted-array/\n",
            ". The given solution in docs is \n",
            "def count_rotation(arr):   \n",
            "    for i in range (1,len(arr)): \n",
            "        if (arr[i] < arr[i - 1]): \n",
            "            return i  \n",
            "    return 0\n",
            ".\n",
            "You are an expert python programmer. Here is your task: \n",
            "Write a python function to count the number of rotations required to generate a sorted array. https://www.geeksforgeeks.org/count-of-rotations-required-to-generate-a-sorted-array/\n",
            " Respond *only* with the python function.No more. Do NOT add any additional text after the function's end:\n",
            "      \n",
            "The output generated is: \n",
            "```python\n",
            "def count_rotation(arr):   \n",
            "    for i in range (1,len(arr)): \n",
            "        if (arr[i] < arr[i - 1]): \n",
            "            return i  \n",
            "    return 0\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Just testing the code output\n",
        "print(f\"\\nThe Task is \\n{mbpp_df['prompt'].iloc[0]}\\n. The given solution in docs is \\n{mbpp_df['code'].iloc[0]}\\n.\")\n",
        "\n",
        "\n",
        "print(f\"The output generated is: \\n{evaluateRAG(mbpp_df['prompt'].iloc[0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eCS9E9mi7HB"
      },
      "source": [
        "## Generate model outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9Kuo918mwvX"
      },
      "outputs": [],
      "source": [
        "model_outputs = []\n",
        "for prompt in mbpp_df[\"prompt\"]:\n",
        "  output = evaluateRAG(prompt)\n",
        "  model_outputs.append(output)\n",
        "\n",
        "model_outputs = pd.Series(model_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7cK6bShLhUw"
      },
      "source": [
        "### Pass/Fail Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqLkNyNdM2Qg",
        "outputId": "66e3f29b-6efe-4eea-bbc1-2709eb9da7dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```python\n",
            "def large_product(nums1, nums2, N):\n",
            "    result = sorted([x*y for x in nums1 for y in nums2], reverse=True)[:N]\n",
            "    return result\n",
            "```\n",
            "assert large_product([1, 2, 3, 4, 5, 6],[3, 6, 8, 9, 10, 6],3)==[60, 54, 50]\n",
            "assert large_product([1, 2, 3, 4, 5, 6],[3, 6, 8, 9, 10, 6],4)==[60, 54, 50, 48]\n",
            "assert large_product([1, 2, 3, 4, 5, 6],[3, 6, 8, 9, 10, 6],5)==[60, 54, 50, 48, 45]\n"
          ]
        }
      ],
      "source": [
        "# Hardcoded script to test pass/fail cases\n",
        "print(model_outputs.iloc[9])\n",
        "for test in mbpp_df[\"test_list\"].iloc[9]:\n",
        "  print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aTXNtlTNoGS",
        "outputId": "b9e363d5-1e6e-4b8a-cd5f-a48e5742ec25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No Assertion Error. All tests Passed\n"
          ]
        }
      ],
      "source": [
        "exec(\"\"\"\n",
        "def large_product(nums1, nums2, N):\n",
        "    result = sorted([x*y for x in nums1 for y in nums2], reverse=True)[:N]\n",
        "    return result\n",
        "\n",
        "assert large_product([1, 2, 3, 4, 5, 6],[3, 6, 8, 9, 10, 6],3)==[60, 54, 50]\n",
        "assert large_product([1, 2, 3, 4, 5, 6],[3, 6, 8, 9, 10, 6],4)==[60, 54, 50, 48]\n",
        "assert large_product([1, 2, 3, 4, 5, 6],[3, 6, 8, 9, 10, 6],5)==[60, 54, 50, 48, 45]\n",
        "\n",
        "print(\"No Assertion Error. All tests Passed\")\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
